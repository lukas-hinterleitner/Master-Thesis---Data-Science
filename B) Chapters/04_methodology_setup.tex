\chapter{Model, Data, and Infrastructure}
This chapter presents the experimental setup for the computations laid out in Chapter~\ref{cha:gradient_similarity}. First, the model and data sources are specified, and the acquisition of third-party artifacts is formalized to ensure reproducibility. The construction of the working datasets is then described, including message formatting and paraphrasing procedures that enable controlled comparisons between the original and transformed samples.
\\\\
To apply the analysis on modern \acrshort{llm}s, three mechanisms are employed: (i) search-space reduction via lexical pre-selection, (ii) storage of intermediate dot products in place of full gradients, and (iii) layer-wise random projections. The hardware and scheduling environment are summarized to make resource assumptions explicit and to facilitate replication. The overall goal is to isolate per-instance effects while keeping the pipeline deterministic and verifiable.

\section{HuggingFace as a General Model and Data Provider}
\label{sec:hf_provider}
In this work, the \emph{HuggingFace} ecosystem serves as the unified provider for models and datasets. Models are retrieved via \emph{transformers} using \texttt{from\_pretrained(\textless model\textgreater)} to obtain versioned weights, configuration files, and tokenizer assets; tokenizers are loaded analogously to ensure consistent preprocessing across all runs~\cite{wolf-etal-2020-transformers}. Datasets are accessed through the \emph{datasets} library, which offers streaming, on-disk caching, dataset cards with licensing information, and standardized schemas for robust iteration and filtering~\cite{lhoest-etal-2021-datasets}.
\\\\
Relying on a single provider simplifies the end-to-end workflow and reduces nondeterminism. In particular:
\begin{itemize}
    \item \textbf{Versioning and Reproducibility}: Pinning a specific revision guarantees that identical artifacts (weights, configs, tokenizer vocabulary, and chat templates) are resolved in subsequent runs.
    \item \textbf{Cached Artifacts}: Local caching enables offline execution and stable re-use of immutable assets without manual bookkeeping.
    \item \textbf{Introspectable Metadata}: Model cards, configuration files, and tokenizer or chat-template metadata (e.g., \texttt{tokenizer.chat\_template}) can be read programmatically to construct inputs aligned with each model’s pretraining or \acrshort{sft} format.
    \item \textbf{Schema-First Data Access}: Standardized dataset schemas and filtering utilities minimize glue code and keep preprocessing auditable.
\end{itemize}
Taken together, these properties ensure that preprocessing, evaluation, and analysis operate over a consistent, fully specified set of artifacts, thereby supporting exact reproducibility of all experiments.

\subsection{OLMo}
The OLMo model series, developed by the \emph{Allen Institute for AI}, provides open-source \acrshort{llm}s along with their training data and comprehensive training pipelines~\cite{Groeneveld2023OLMo}. These features make OLMo models particularly suitable for research focused on explainability. Specifically, OLMo models are pre-trained on the \emph{Dolma} dataset, which comprises approximately three trillion tokens collected from diverse sources~\cite{dolma}. To adapt these models to specific tasks, the \emph{Open Instruct} framework combined with the \emph{TÜLU} dataset is employed for fine-tuning~\cite{ivison2023camels}.
\\\\
In this article, the focus is on the \texttt{AMD-OLMo-1B-SFT} model from AMD, which is derived from the original \texttt{OLMo-1B} architecture, as detailed in Table~\ref{tab:model_architecture_olmo_1b_amd}. This specific model was pre-trained using the \emph{Dolma v1.7} dataset, consisting of 1.3 trillion tokens, and subsequently fine-tuned through supervised training on the \emph{TÜLU v2} dataset~\cite{AMD-OLMo}.
\begin{table}[htb]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline  
        \BreakHeader{Parameter \\ size} & \BreakHeader{Number of \\ layers} & \BreakHeader{Number of \\ heads} & \BreakHeader{Hidden \\ size} & \BreakHeader{Context \\ length} & \BreakHeader{Vocabulary \\ Size} \\ 
        \hline  
        1.2B & 16 & 16 & 2{,}048 & 2{,}048 & 50{,}280 \\
        \hline
    \end{tabular}
    \caption{Model architecture size of \texttt{AMD-OLMo-1B-SFT} \cite{AMD-OLMo}}
    \label{tab:model_architecture_olmo_1b_amd}
\end{table} 
The model is loaded using \emph{HuggingFace}\textquotesingle s \emph{transformers} function \texttt{from\_pretrained(\textless model\textgreater)} from the class \texttt{AutoModelForCausal}, where \texttt{model} is the model name (\texttt{AMD-OLMo-1B-SFT}). It allows published models with corresponding pre-trained weights to be downloaded and further adapted using \emph{PyTorch}. The complete model architecture when loaded with the \emph{transformers} and printed on the console looks like:
\begin{verbatim}
OlmoForCausalLM(
  (model): OlmoModel(
    (embed_tokens): Embedding(50304, 2048, padding_idx=1)
    (layers): ModuleList(
      (0-15): 16 x OlmoDecoderLayer(
        (self_attn): OlmoAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        )
        (mlp): OlmoMLP(
          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): OlmoLayerNorm()
        (post_attention_layernorm): OlmoLayerNorm()
      )
    )
    (norm): OlmoLayerNorm()
    (rotary_emb): OlmoRotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)
)
\end{verbatim}
Table~\ref{tab:model_architecture_olmo_1b_amd} shows a vocabulary size of $50{,}280$, while the actual model output shows a size of $50{,}304$. The authors increased the size to be a multiple of $128$ to maximize training throughput \cite{Groeneveld2023OLMo}. As shown above, the model consists of the following trainable components.
\begin{itemize}
    \item Input Embedding: $\mathbf{E} \in \mathbb{R}^{|V| \times d_x}$
    \item 16 Decoder Layers ($l = 0, \ldots, 15$):
    \begin{itemize}
        \item Attention:
        \begin{itemize}
            \item Query-Projection:   $\mathbf{W}_\text{Q}^{(l)} \in \mathbb{R}^{d_x \times d_x}$
            \item Key-Projection:     $\mathbf{W}_\text{K}^{(l)} \in \mathbb{R}^{d_x \times d_x}$
            \item Value-Projection:   $\mathbf{W}_\text{V}^{(l)} \in \mathbb{R}^{d_x \times d_x}$
            \item Output-Projection:  $\mathbf{W}_\text{O}^{(l)} \in \mathbb{R}^{d_x \times d_x}$
        \end{itemize}
        \item \acrshort{mlp}:
        \begin{itemize}
            \item Gate-Projection:    $\mathbf{W}_\text{gate}^{(l)} \in \mathbb{R}^{d_x \times d_{ff}}$
            \item Up-Projection:      $\mathbf{W}_\text{up}^{(l)} \in \mathbb{R}^{d_x \times d_{ff}}$
            \item Down-Projection:    $\mathbf{W}_\text{down}^{(l)} \in \mathbb{R}^{d_{ff} \times d_x}$
        \end{itemize}
    \end{itemize}
    \item \acrshort{lm} Head: $\mathbf{W}^{(o)} \in \mathbb{R}^{d_x \times |V|}$
\end{itemize}
The dimensionalities $|V|$, $d_x$, and $d_{ff}$ used in the above weight notation correspond to the architecture parameters, as specified below:
\begin{itemize}
    \item $|V|$: Vocabulary size (for \texttt{AMD-OLMo-1B-SFT}, $|V| = 50{,}304$).
    \item $d_x$: Hidden size (for \texttt{AMD-OLMo-1B-SFT}, $d_x = 2{,}048$).
    \item $d_{ff}$: Feedforward size (for \texttt{AMD-OLMo-1B-SFT}, $d_{ff} = 8{,}192$).
\end{itemize}
The total number of parameters $M_H$ for \texttt{AMD-OLMo-1B-SFT} with $L = 16$ decoder layers is:
\begin{align*}
    M_H &=\quad \underbrace{|V| \cdot d_x}_{\text{Input Embedding}} 
    + \underbrace{L \big( 4 d_x^2 + 3 d_x d_{ff} \big)}_{\text{Decoder Layers}}
    + \underbrace{d_x \cdot |V|}_{\text{LM Head}} \\
    &=\quad 2 \bigl( |V| \cdot d_x \bigr)
    + L \big( 4 d_x^2 + 3 d_x d_{ff} \big) \\
    &=\quad 2 \cdot 50{,}304 \cdot 2{,}048 + 16 \bigl(4 \cdot 2{,}048^2 + 3 \cdot 2{,}048 \cdot 8{,}192 \bigr) \\
    &=\quad 1{,}279{,}787{,}008
\end{align*}
However, this number does not match the model output $M = 1{,}176{,}764{,}416$ when calling the function \texttt{model.num\_parameters()}. The difference $\Delta_M = M_H - M = 103{,}022{,}592$ happens to be exactly $\Delta_M = |V| \cdot d_x = 50{,}304 \cdot 2{,}048$ which is the number of parameters for the embedding. This indicates some form of weight-tying, and when looking into the model configuration and the model source code \cite{Groeneveld2023OLMo}, this proves to be true\footnote{HuggingFace configuration: \url{https://huggingface.co/amd/AMD-OLMo-1B-SFT/blob/main/config.json\#L21}}. Hence, the final number of trainable parameters is $M$ since the \acrshort{lm} head matrix $\mathbf{W}^{(o)} \in \mathbb{R}^{d_x \times |V|}$ is just a form of transposed input embedding matrix $\mathbf{E} \in \mathbb{R}^{|V| \times d_x}$. A visual representation of the trainable parameters per layer is shown in Figure~\ref{fig:parameters_per_layer_amd_olmo}. In summary, each layer component has a fixed number of trainable parameters as illustrated in Table~\ref{tab:parameter_components_olmo}.
\begin{table}[htb]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Component} & \textbf{Shape} & \textbf{Number of Parameters} \\
        \hline
        Input Embedding & $\mathbb{R}^{|V| \times d_x}$ & $|V| \cdot d_x = 50{,}304 \cdot 2{,}048 = 103{,}022{,}592$ \\
        Attention Projections & $\mathbb{R}^{d_x \times d_x}$ & $2{,}048^2 = 4{,}194{,}304$ \\
        \acrshort{mlp} Projections & $\mathbb{R}^{d_x \times d_{ff}}$; $\mathbb{R}^{d_{ff} \times d_x}$ & $2{,}048 \cdot 8{,}192 = 16{,}777{,}216$ \\
        \hline
    \end{tabular}
    \caption{Trainable parameters in AMD-OLMo-1B-SFT per layer components.}
    \label{tab:parameter_components_olmo}
\end{table}

\subsection{LIMA Dataset}\label{subsec:lima_dataset}
The \emph{LIMA} dataset is a subset of the \emph{TÜLU} dataset, containing 1,000 carefully curated prompts and responses across various domains, including STEM fields, alongside manually authored samples \cite{zhou2023limaalignment}. Similar to other instruction-tuning datasets, it comprises pairs of prompts and corresponding generations. After the transformations stated Section~\ref{sec:preparing_the_data}, it will form the original dataset $D$. in A prompt typically represents a user's instruction or query, specifying the task or information the model should provide. The model's output, referred to as a generation or response, directly addresses the user's prompt. An illustrative example $(s_{0_P},s_{0_G})=s_0 \in D$ from the \emph{LIMA} dataset is provided below. This example shows how much quality and depth the \emph{LIMA} dataset has. In this case, the example is also well-referenced and closely resembles an expert-level response.
\\\\
User prompt $s_{0_P}$:
\begin{verbatim}
    Can brain cells move? By movement I mean long distance migration 
    (preferably within the brain only).
\end{verbatim}
Model-generation $s_{0_G}$:
\begin{verbatim}
    The question is relatively broad and one should take into account that 
    the brain not only consists of neurons, but also glial cells 
    (supportive cells) and pre-mitotic neuronal stem cells. Furthermore,
    as critical fellow-scientists have indicated, developmental stage is
    very important, as the developing embryonic brain is very different 
    from the adult brain.\nHowever, after sifting through various 
    publications, the answer to the question is actually remarkably 
    simple: Yes, brain cells migrate.\nIn  the adult brain glial cells
    migrate in the brain (Klämbt, 2009). Glial cells are involved in a
    myriad of functions, but a notable example of migrating glial cells
    are the oligodendrocytes that migrate relative long distances to 
    find their target axons onto which they wrap themselves to form the
    insulating myelin sheath (Tsai and Miller, 2002).\nNeuronal stem cells
    migrate over long distances in response to injury (Imitola et al., 2004)
    and they migrate from specific stem-cell locations (e.g., hippocampus 
    and subventricular zone) to other regions (Clarke, 2003).\nPost-mitotic,
    but non-differentiated neurons have been shown to migrate in the adult 
    brain in fish (Scott et al., 2012), and in mammals and non-human 
    primates as well (Sawada et al., 2011).\nNot surprisingly, glial cells, 
    stem cells and neurons also migrate during embryonic development. 
    Most notably, post-mitotic neurons destined to fulfill peripheral 
    functions have to migrate over relatively long distances from the 
    neural crest to their target locations (Neuroscience, 2nd ed, 
    Neuronal Migration).
\end{verbatim}

\section{Preparing the Data}\label{sec:preparing_the_data}
A small portion of the examples contain complete dialogues with multiple prompts and corresponding responses. These are removed from the dataset to keep the classic prompt/generation theme for all examples and to make them comparable. When removing those dialogue examples, the filtered dataset still consists of $N = 988$ query/response pairs forming the original dataset $D$.

\subsection{User/Assistant Message Format}\label{subsec:message_format}
To ensure consistency and compatibility with modern conversational AI frameworks, each example in the dataset is represented as a list of message objects. Every message specifies both a \texttt{role}, indicating whether it originates from the user, the assistant, or optionally the system, and a \texttt{content} field containing the text.As stated previously, the dialogue examples have been filtered to retain exactly one user-assistant exchange per sample, enforcing a single-turn instruction-response structure for all pairs.

\subsubsection{Roles and Their Purpose}
\begin{itemize}
\item \textbf{system}: An optional message that provides general context or high-level instruction, such as persona or tone guidelines.
\item \textbf{user}: Represents the prompt or question, serving as the input for which the model must generate a response.
\item \textbf{assistant}: Contains the model's response to the user's prompt; this serves as the target output during both training and evaluation.
\end{itemize}

\subsubsection{Dataset Schema}
Each data point is stored in JSON format as a dictionary with a single \texttt{messages} key, which holds a list of message objects. A typical single-turn user-assistant interaction is structured as follows.
\begin{lstlisting}[caption={User–assistant message pair}]
{"messages": [
    { "role": "user", "content": "<user prompt>" },
    { "role": "assistant", "content": "<model output>" }
]}
\end{lstlisting}
This uniform structure facilitates both supervised fine-tuning and later evaluation using the same message format.

\subsection{Paraphrasing Samples}\label{sec:paraphrasing_samples}
As already mentioned in Section~\ref{sec:paraphrasing_a_dataset}, the covered approach operates with two different settings, namely \emph{paraphrased} and \emph{model-generated} paraphrasing levels. For the \emph{paraphrased} case, both the query $s_{i_P}$ (user message) and the generation $s_{i_G}$ (assistant message) are altered using \emph{OpenAI GPT-4o-mini} with a seed of $42$ and a temperature of $1$. The rest of the parameters are set to default values. This process in denoted as $\operatorname{LLM_{para}}(\cdot)$ in Equation~\ref{eq:paraphrasing}. The corresponding system prompt is illustrated below.
\\\\
System prompt for $\operatorname{LLM_{para}}(\cdot)$:
\begin{verbatim}
    You are a paraphrasing expert who is specialized in rewriting text 
    (questions, statements, etc.) without altering the content. 
    Keep in mind, that the meaning must not change after the paraphrasing. 
    Just output the paraphrased text without any additional information.
\end{verbatim}
The \emph{OpenAI} API also follows the message format shown in Subsection~\ref{subsec:message_format}. Consequently, when paraphrasing the example $s_{0_P}$ from Subsection~\ref{subsec:lima_dataset}, the \emph{OpenAI} API receives a system-user message pair object containing the previously illustrated system prompt and the text to be rephrased. Hence, all the samples and therefore, each user and assistant message from the filtered \emph{LIMA} dataset are individually sent to \emph{OpenAI} to be paraphrased. For example, the paraphrase of the the user prompt $s_{0_P}$ shown in Subsection~\ref{subsec:lima_dataset} looks like follows.
\\\\
Paraphrased user prompt $\operatorname{LLM_{para}}(s_{0_P}) = s_{p_{0_P}}$:
\begin{verbatim}
    Are brain cells capable of moving? Specifically, I’m referring to
    long-distance migration, ideally occurring within the brain.
\end{verbatim}
Section~\ref{subsec:appendix_paraphrased_sample} additionally illustrates the paraphrases for the messages shown in Subsection~\ref{subsec:lima_dataset}. This procedure is then repeated for all samples $\si$ in $D$ to create the dataset $D_p$ mentioned in Section~\ref{sec:paraphrasing_a_dataset} and Equation~\ref{eq:paraphrasing}. To create the dataset $D_m$ for the \emph{model-generated} case, the paraphrased user messages from $D_p$ are input to the model under evaluation (\texttt{AMD-OLMo-1B-SFT}). To create a sample $\smi \in D_m$, a paraphrased user message $s_{p_{i_P}}$ is piped through the model ($\operatorname{LLM}(\cdot)$ to generate a new assistant message $s_{m_{i_G}}$ as first shown in Equation~\ref{eq:model_generated_sample}. Subsection~\ref{subsec:prepare_model_input} shows how this is done on a more technical level.

\subsection{Prepare Model-generated Data Point}\label{subsec:prepare_model_input}
Before the user message can be picked up by the model, some modifications are made to the sample. Usually, the user message is embedded inside a form of tag that tells the model where the user message starts and ends. How these tags are formatted and applied differs from model to model in the corresponding training pipeline. In the case of \texttt{AMD-OLMo-1B-SFT}, the user prompt $s_{0_P}$ after formatting looks like:
\begin{verbatim}
    <|user|>\n
    Are brain cells capable of moving? 
    Specifically, I'm referring to long-distance migration, 
    ideally occurring within the brain.\n
    <|assistant|>\n
\end{verbatim}
The \texttt{<|user|>} tag tells the model where the user message starts. The \texttt{<|assistant|>} tag tells the model where the user message ends and that the model should start writing the response. In the space of HuggingFace, this kind of format is usually called \emph{chat template}. After applying this template, the text is tokenized and converted to a numerical format by a so-called \emph{tokenizer} to make it readable for the model. For \texttt{AMD-OLMo-1B-SFT}, the tokenized and converted user prompt $s_{0_P}$ sequence looks like the following:
\begin{verbatim}
     29, 93, 4537, 49651, 187, 6723, 3998, 1341, 7032, 273,
     4886, 32, 13658, 13, 309, 1353, 14339, 281, 1048, 14,
     19893, 10346, 13, 34243, 12952, 1561, 253, 3998, 15, 187,
     29, 93, 515, 5567, 49651, 187
\end{verbatim}
Each number represents one token; for example, the value $187$ encodes the term \texttt{\textbackslash n}. This sequence of tokens is used to generate a response, which also represents a sequence of tokens. This sequence is then decoded using the same \emph{tokenizer} as before and yields the model-generated assistant message $s_{m_{0_G}}$. Section~\ref{subsec:appendix_model_generated_sample} illustrates the decoded answer () for the example shown above.
\\\\
This process is then repeated for each sample from the \emph{original} dataset\footnote{Original dataset: \url{https://huggingface.co/datasets/lukashinterleitner/LIMA-original}} to generate the \emph{paraphrased} dataset\footnote{Paraphrased dataset: \url{https://huggingface.co/datasets/lukashinterleitner/LIMA-paraphrased-GPT-4o-mini}} $D_p$ and the \emph{model-generated} dataset\footnote{Model-generated dataset: \url{https://huggingface.co/datasets/lukashinterleitner/LIMA-model-generated-amd-AMD-OLMo-1B-SFT}} $D_m$. These \emph{paraphrased} and \emph{model-generated} examples are stored on disk and picked up when needed to guarantee a level of determinism when operating on this newly created dataset.

\subsection{Supervised Fine-Tuning Setup}
Although no additional \acrfull{sft} is performed in this work, all data samples are prepared according to established \acrshort{sft} conventions. This ensures full compatibility with existing model evaluation pipelines and facilitates the subsequent analysis of model behavior from an explainability perspective. Specifically, each sample is formatted as a user–assistant message pair, following the schema described in previous sections.

\subsubsection{Open-Instruct}
In the context of this work, \emph{Open-Instruct} is mainly utilized to preprocess and structure the data according to established \acrshort{sft} conventions. Specifically, the framework offers utilities for converting raw user–assistant-message pairs such as those from the \emph{LIMA} dataset into the standardized chat template format (e.g., \texttt{<|user|>} and \texttt{<|assistant|>} tags) used by models like \texttt{AMD-OLMo-1B-SFT}. This ensures consistency between the data preparation procedure and the input expectations of the underlying model architecture.
\\\\
To preprocess the data for \acrfull{sft}, \emph{SFTDatasetProcessor} from \emph{Open-Instruct} is used. It allows a whole dataset to be tokenized and prepared by forming the corresponding input and output sequences and naturally integrating into the \emph{HuggingFace} ecosystem for \acrshort{sft}.

\section{Instance- and Gradient-based Explanation}
Having established the theoretical framework for gradient similarity in the previous chapter, this section illustrates the practical implementation of this method. The core idea is to leverage the loss function gradient with respect to individual data samples to identify influential training examples. As established in the fine-tuning setting (Section~\ref{sec:fine_tuning_setting}), all gradient computations are performed on single instances (i.e., with a batch size of one) to isolate the influence of individual samples. However, applying this concept to modern \acrlong{llm}s introduces significant computational and memory-related challenges. The following subsections shed light on the strategies employed to make this analysis manageable, addressing the high dimensionality of gradients, implementing an efficient candidate selection process, and utilizing intermediate results to manage the computational load.

\subsection{High Dimensionality}\label{subsec:high_dimensionality}
The primary challenge in applying the similarity of the gradient to modern \acrlong{llm}s is the dimensionality of the parameter space. For the \texttt{AMD-OLMo-1B-SFT} model used in this work, the full model gradient $\vXsi[\theta]$ for sample $\si$, as defined in the gradient flattening process (Subsection~\ref{subsec:gradient_flattening}), is a vector in $\mathbb{R}^{M}$, where $M = 1{,}176{,}764{,}416$. Storing even a single such gradient vector, assuming 32-bit floating point precision (4 bytes per parameter), would require $1{,}176{,}764{,}416 \cdot 4$ bytes $= 4{,}707{,}057{,}664$ bytes $\approx 4.7$ GB of memory or disk space.
\\\\
A naive approach might involve caching the full gradient vector for each sample in the datasets $D$ and $D_p$, introduced in Section~\ref{sec:fine_tuning_setting} and Section~\ref{sec:paraphrasing_a_dataset}, respectively. However, for the filtered \emph{LIMA} dataset with $N=988$ samples, this would amount to storing nearly $2 \cdot 988 \cdot 4.7$ GB $\approx 9.3$ TB of data, which is practically infeasible for most research environments. When adding the model-generated dataset $D_m$, this increases to $\approx 14$ TB of data. Consequently, storing the full gradient vectors for each sample is not a viable strategy, necessitating an alternative that avoids materializing these high-dimensional objects altogether. How this problem is approached, will be clarified in the next few sections.

\subsection{Efficient Candidate Selection with BM25}\label{subsec:setup_bm25}
A naive implementation of the accuracy metric in Equation~\ref{eq:accuracy_full} would require computing the cosine similarity between each paraphrased sample gradient $\vXspi$ and every original sample gradient $\vXsj$. For the working dataset of size $N=988$, this results in a computationally intensive $N \times N$ comparison matrix of nearly one million pairs. However, preliminary analyses on smaller subsets revealed that for a given paraphrased sample $\spi$, its original counterpart $\si$ consistently produced one of the highest similarity scores among all possible pairs. This suggests that the full $N \times N$ comparison is largely redundant.
\\\\
To drastically reduce execution time, the BM25 ranking function, introduced in Subsection~\ref{subsec:bm25_selection}, is used to pre-filter the comparison set. As detailed in Algorithm~\ref{alg:bm25_select_samples}, instead of comparing against the entire original dataset $D$, a small candidate set $\mathcal{C}_i(b)$ is first identified, containing the $b$ most lexically similar samples for each $\spi$. For all experiments conducted in this work, this candidate size is set to $b=5$, a value deemed sufficient based on preliminary findings. This targeted approach reduces the number of expensive gradient similarity computations from $988 \times 988 = 976{,}144$ to just $988 \times 5 = 4940$ comparisons per dataset, making the overall accuracy calculation tractable without sacrificing meaningful comparisons.

\subsection{Intermediate Results}\label{subsec:intermediate_results}
Even with the reduced comparison set, the challenge of handling enormous gradient vectors during computation remains. The solution lies in recognizing that cosine similarity, as shown in Equations~\ref{eq:cosine_dot_full} through~\ref{eq:cosine_dot_subset}, is constructed entirely from dot products. Instead of calculating and storing the full, prohibitively large gradient vectors, only the constituent dot products for each layer component are computed and stored.
\\\\
For each pair of relevant samples $(\spi, \sj)$ where $j \in \mathcal{C}_i(b)$, the following dot products are calculated:
\begin{itemize}
    \item $\vXspi[\Wlk]^{\top}\vXsj[\Wlk]$ \quad (\emph{cross dot products})
    \item $\vXspi[\Wlk]^{\top}\vXspi[\Wlk]$ \quad (\emph{paraphrased dot products})
    \item $\vXsj[\Wlk]^{\top}\vXsj[\Wlk]$ \quad (\emph{original dot products})
\end{itemize}
This is done for every layer component $\Wlk$ in the model. These scalar values are lightweight and can be efficiently stored on disk. They serve as precomputed intermediate results that can be aggregated later to reconstruct the cosine similarity for any combination of layer components, as required by the Greedy Layer Selection algorithm (Section~\ref{sec:greedy_layer_selection}) and other analysis tasks. This strategy completely circumvents the need to store full gradients, thereby resolving the high-dimensionality bottleneck when storing intermediate results.
\\\\
All intermediate results are stored as \emph{JSON} files to retain the sample ids for later analysis. The three different dot products (\emph{cross dot products}, etc.) stated above are stored in three different files and later combined accordingly. A shortened example of the \emph{cross dot products} file for \texttt{AMD-OLMo-1B-SFT} is illustrated below.
\begin{verbatim}
{
"lima_0": {
    "lima_0": {
        "model.embed_tokens.weight": 2.129183530807495,
        "model.layers.0.self_attn.q_proj.weight": 0.08331464231014252,
        "model.layers.0.self_attn.k_proj.weight": 0.12357588112354279,
        ...
    },
    "lima_451": {
        "model.embed_tokens.weight": -0.08331018686294556,
        "model.layers.0.self_attn.q_proj.weight": -0.01116477232426405,
        "model.layers.0.self_attn.k_proj.weight": -0.009237218648195267,
        ...
    },
    ...
},
"lima_1": {
    "lima_1": {
        "model.embed_tokens.weight": 1.2379783391952515,
        "model.layers.0.self_attn.q_proj.weight": 0.027665752917528152,
        ...
    },
    ...
},
...
}
\end{verbatim}

\subsection{Random Projection}
The core challenge in applying random projection directly is the prohibitive size of the projection matrix $\mathbf{R}$. For a model with $M \approx 1.18$ billion parameters, a projection to a 1\% dimension results in a target dimension of $d = 0.01 \cdot M \approx 11.8$ million. The full projection matrix $\mathbf{R}$ would therefore have dimensions of approximately $11.8 \text{ million} \cdot 1.18 \text{ billion}$. Storing this matrix with 32-bit precision (4 bytes per element) would require approximately $11.8 \cdot 10^6 \cdot 1.18 \cdot 10^9 \cdot 4$ bytes $= 5.5696 \cdot 10^{16}$ bytes, a size on the order of tens of petabytes, which is computationally infeasible to generate or store in memory. Therefore, a layer-wise projection strategy is adopted, leveraging the efficient \texttt{CudaProjector} from the \texttt{trak} library~\cite{park2023trak}. This approach avoids constructing the full projection matrix at once.
\\\\
The total projection dimension for each configuration is distributed across the model's layer components proportionally to their size. For a given total projection dimension $d_\text{proj}$, the dimension for each individual component $(l,k)$, denoted $d_{(l,k)}$, is calculated as shown in Algorithm~\ref{alg:proportional_dim_allocation}.
\begin{algorithm}[ht]
\caption{Proportional Allocation of Projection Dimensions to Layer Components}
\label{alg:proportional_dim_allocation}
\SetKwInOut{Data}{Data}
\SetKwInOut{Result}{Result}
\DontPrintSemicolon

\Data{
Parameter counts $P_{(l,k)}$ for all layer components $(l,k) \in \mathcal{W}$;
Total parameter count $M = \sum_{(l,k) \in \mathcal{W}} P_{(l,k)}$;
Target total projection dimension $d_\text{proj}$;
Minimal component dimension $d_\text{min} \gets 512$;
Rounding granularity $g \gets 512$;
}
\Result{Projection dimension $d_{(l,k)}$ assigned to each component, $(l,k) \in \mathcal{W}$}
\ForEach{$(l,k) \in \mathcal{W}$}{
$p_{(l,k)} \gets \frac{P_{(l,k)}}{M}$; \tcp*{Fraction of parameters in component $(l,k)$}
$d_\text{raw} \gets p_{(l,k)} \cdot d_\text{proj}$; \tcp*{Unrounded allocation}
$d_\text{rounded} \gets g \cdot \operatorname{round} \left(\frac{d_\text{raw}}{g}\right)$; \tcp*{Round to nearest multiple of $g$}
$d_{(l,k)} \gets \max(d_\text{min}, d_\text{rounded})$; \tcp*{Enforce minimal size}
}
\Return{$d_{(l,k)}\ |\ (l,k)\in\mathcal{W}$}
\end{algorithm}
\\\\
This method ensures that larger layers are projected to correspondingly larger-dimensional spaces, preserving more of their geometric information. The rounding to a multiple of 512 is a practical choice for computational efficiency on modern GPU architectures. The result is a set of projected layer gradients that are concatenated to form the final low-dimensional representation for similarity comparison.
\\\\
For the benchmark, two random projection configurations were used, with the total target
dimensions set to 1\% and 5\% of the total number of model parameters, effectively leading to $11767808$ and $58838016$. The random projection matrix is sampled from a \emph{Rademacher} distribution provided by the \texttt{trak} library~\cite{park2023trak}.

\section{Hardware}\label{sec:hardware}
To mitigate long-running tasks, all computations were executed on a \emph{Slurm} cluster with a \emph{NVIDIA DGX H100} compute node which consists of 8x NVIDIA H100 Tensor Core GPUs, 2 TB RAM and Dual Intel® Xeon® Platinum 8480C Processors with
112 Cores total, 2.00 GHz (Base), 3.80 GHz (Max Boost). 
\\\\
Before starting the calculations, the dataset is sliced into $P_D$ partitions. Each partition is then assigned to a batch job and queued if necessary. Each batch job got assigned a fixed amount of resources. Hence, each job effectively had access to one NVIDIA H100 Tensor Core GPU, 86 GB RAM, and eight cores.