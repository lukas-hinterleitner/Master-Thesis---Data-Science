\chapter{TBR: Methodology - Experimental Setup}
\fxnote{TODO: introduction text}

\section{HuggingFace as a General Model and Data Provider}
asdf

\subsection{OLMo}
The OLMo model series, developed by the \emph{Allen Institute for AI}, provides open-source \acrshort{llm}s along with their training data and comprehensive training pipelines~\cite{Groeneveld2023OLMo}. These features make OLMo models particularly suitable for research focused on explainability. Specifically, OLMo models are pre-trained on the \emph{Dolma} dataset, which comprises approximately three trillion tokens collected from diverse sources~\cite{dolma}. To adapt these models to specific tasks, the \emph{Open Instruct} framework combined with the \emph{TÜLU} dataset is employed for fine-tuning~\cite{ivison2023camels}.
\\\\
In this article, the focus is on the \texttt{AMD-OLMo-1B-SFT} model from AMD, which is derived from the original \texttt{OLMo-1B} architecture, as detailed in Table~\ref{tab:model_architecture_olmo_1b_amd}. This specific model was pre-trained using the \emph{Dolma v1.7} dataset, consisting of 1.3 trillion tokens, and subsequently fine-tuned through supervised training on the \emph{TÜLU v2} dataset~\cite{AMD-OLMo}.
\begin{table}[htb]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline  
        \BreakHeader{Parameter \\ size} & \BreakHeader{Number of \\ layers} & \BreakHeader{Number of \\ heads} & \BreakHeader{Hidden \\ size} & \BreakHeader{Context \\ length} & \BreakHeader{Vocabulary \\ Size} \\ 
        \hline  
        1.2B & 16 & 16 & 2{,}048 & 2{,}048 & 50{,}280 \\
        \hline
    \end{tabular}
    \caption{Model architecture size of AMD-OLMo-1B-SFT \cite{AMD-OLMo}}
    \label{tab:model_architecture_olmo_1b_amd}
\end{table} 
The model is loaded using \emph{HuggingFace}\textquotesingle s \emph{transformers} function \texttt{from\_pretrained(\textless model\textgreater)} from the class \texttt{AutoModelForCausal}, where \texttt{model} is the model name (\texttt{AMD-OLMo-1B-SFT}). It allows published models with corresponding pre-trained weights to be downloaded and further adapted using \emph{PyTorch}. The complete model architecture when loaded with the \emph{transformers} and printed on the console looks like:
\begin{verbatim}
OlmoForCausalLM(
  (model): OlmoModel(
    (embed_tokens): Embedding(50304, 2048, padding_idx=1)
    (layers): ModuleList(
      (0-15): 16 x OlmoDecoderLayer(
        (self_attn): OlmoAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        )
        (mlp): OlmoMLP(
          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): OlmoLayerNorm()
        (post_attention_layernorm): OlmoLayerNorm()
      )
    )
    (norm): OlmoLayerNorm()
    (rotary_emb): OlmoRotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)
)
\end{verbatim}
Table~\ref{tab:model_architecture_olmo_1b_amd} shows a vocabulary size of $50{,}280$, while the actual model output shows a size of $50{,}304$. The authors increased the size to be a multiple of $128$ to maximize training throughput \cite{Groeneveld2023OLMo}. As shown above, the model consists of the following trainable components.
\begin{itemize}
    \item Input Embedding: $\mathbf{E} \in \mathbb{R}^{|V| \times d_x}$
    \item 16 Decoder Layers ($l = 0, \ldots, 15$):
    
    \hspace*{2em}
    \begin{tabular}{@{}ll}
        - Attention: & \\[0.5em]
        \quad - Query-Projection:     & $\mathbf{W}_\text{Q}^{(l)} \in \mathbb{R}^{d_x \times d_x}$ \\[0.5em]
        \quad - Key-Projection:       & $\mathbf{W}_\text{K}^{(l)} \in \mathbb{R}^{d_x \times d_x}$ \\[0.5em]
        \quad - Value-Projection:     & $\mathbf{W}_\text{V}^{(l)} \in \mathbb{R}^{d_x \times d_x}$ \\[0.5em]
        \quad - Output-Projection:    & $\mathbf{W}_\text{O}^{(l)} \in \mathbb{R}^{d_x \times d_x}$ \\[0.5em]
        - \acrshort{mlp}: & \\[0.5em]
        \quad - Gate-Projection:      & $\mathbf{W}_\text{gate}^{(l)} \in \mathbb{R}^{d_x \times d_{ff}}$ \\[0.5em]
        \quad - Up-Projection:        & $\mathbf{W}_\text{up}^{(l)} \in \mathbb{R}^{d_x \times d_{ff}}$ \\[0.5em]
        \quad - Down-Projection:      & $\mathbf{W}_\text{down}^{(l)} \in \mathbb{R}^{d_{ff} \times d_x}$ \\[0.5em]
    \end{tabular}
    
    \item \acrshort{lm} Head: $\mathbf{W}^{(o)} \in \mathbb{R}^{d_x \times |V|}$
\end{itemize}
The dimensionalities $|V|$, $d_x$, and $d_{ff}$ used in the above weight notation correspond to the architecture parameters, as specified below:
\begin{itemize}
    \item $|V|$: Vocabulary size (for \texttt{AMD-OLMo-1B-SFT}, $|V| = 50{,}304$).
    \item $d_x$: Hidden size (for \texttt{AMD-OLMo-1B-SFT}, $d_x = 2{,}048$).
    \item $d_{ff}$: Feedforward size (for \texttt{AMD-OLMo-1B-SFT}, $d_{ff} = 8{,}192$).
\end{itemize}
The total number of trainable parameters $M$ for \texttt{AMD-OLMo-1B-SFT} with $L = 16$ decoder layers is:
\begin{align*}
    M &=\quad \underbrace{|V| \cdot d_x}_{\text{Input Embedding}} 
    + \underbrace{L \big( 4 d_x^2 + 3 d_x d_{ff} \big)}_{\text{Decoder Layers}}
    + \underbrace{d_x \cdot |V|}_{\text{LM Head}} \\
    &=\quad 2 \bigl( |V| \cdot d_x \bigr)
    + L \big( 4 d_x^2 + 3 d_x d_{ff} \big) \\
    &=\quad 2 \cdot 50{,}304 \cdot 2{,}048 + 16 \bigl(4 \cdot 2{,}048^2 + 3 \cdot 2{,}048 \cdot 8{,}192 \bigr) \\
    &=\quad 1279787008
\end{align*}
However, this number does not match the model output $M_H = 1176764416$ when calling the function \texttt{model.num\_parameters()}. The difference $\Delta_M =  M - M_H = 103022592$ happens to be exactly $\Delta_M = |V| \cdot d_x = 50{,}304 \cdot 2{,}048$. This indicates some form of weight-tying, and when looking into the model configuration this proves to be true.
\\\\
how to reference tied embedding urls:
\url{https://github.com/allenai/OLMo/blob/04820704616af5d25cdba4df23aa7b4d9ce86cad/configs/official-0724/OLMo-1B.yaml#L14} \\
\url{https://huggingface.co/amd/AMD-OLMo-1B-SFT/blob/main/config.json#L21}

\subsection{LIMA Dataset}\label{subsec:lima_dataset}
The \emph{LIMA} dataset is a subset of the \emph{TÜLU} dataset, containing 1,000 carefully curated prompts and responses across various domains, including STEM fields, alongside manually authored samples \cite{zhou2023limaalignment}. Similar to other instruction-tuning datasets, it comprises pairs of prompts and corresponding generations. A prompt typically represents a user's instruction or query, specifying the task or information the model should provide. The model's output, referred to as a generation or response, directly addresses the user's prompt. An illustrative example from the \emph{LIMA} dataset is provided below. This example shows how much quality and depth the \emph{LIMA} dataset has. In this case, the example is well-referenced and closely resembles an expert-level response.
\\\\
User prompt $s_{0_P}$:
\begin{verbatim}
    Can brain cells move? By movement I mean long distance migration 
    (preferably within the brain only).
\end{verbatim}
Model-generation $s_{0_G}$:
\begin{verbatim}
    The question is relatively broad and one should take into account that 
    the brain not only consists of neurons, but also glial cells 
    (supportive cells) and pre-mitotic neuronal stem cells. Furthermore,
    as critical fellow-scientists have indicated, developmental stage is
    very important, as the developing embryonic brain is very different 
    from the adult brain.\nHowever, after sifting through various 
    publications, the answer to the question is actually remarkably 
    simple: Yes, brain cells migrate.\nIn  the adult brain glial cells
    migrate in the brain (Klämbt, 2009). Glial cells are involved in a
    myriad of functions, but a notable example of migrating glial cells
    are the oligodendrocytes that migrate relative long distances to 
    find their target axons onto which they wrap themselves to form the
    insulating myelin sheath (Tsai and Miller, 2002).\nNeuronal stem cells
    migrate over long distances in response to injury (Imitola et al., 2004)
    and they migrate from specific stem-cell locations (e.g., hippocampus 
    and subventricular zone) to other regions (Clarke, 2003).\nPost-mitotic,
    but non-differentiated neurons have been shown to migrate in the adult 
    brain in fish (Scott et al., 2012), and in mammals and non-human 
    primates as well (Sawada et al., 2011).\nNot surprisingly, glial cells, 
    stem cells and neurons also migrate during embryonic development. 
    Most notably, post-mitotic neurons destined to fulfill peripheral 
    functions have to migrate over relatively long distances from the 
    neural crest to their target locations (Neuroscience, 2nd ed, 
    Neuronal Migration).
\end{verbatim}

\section{Preparing the Data}
A small portion of the examples contain complete dialogues with multiple prompts and corresponding responses. These are removed from the dataset to keep the classic prompt/generation theme for all examples and to make them comparable. When removing those dialogue examples, the filtered dataset still consists of $N = 988$ query/response pairs.

\subsection{User/Assistant Message Format}\label{subsec:message_format}

\subsubsection{General Overview}
To ensure consistency and compatibility with modern conversational AI frameworks, each example in the dataset is represented as a list of message objects. Every message specifies both a \texttt{role}, indicating whether it originates from the user, the assistant, or optionally the system, and a \texttt{content} field containing the text.As stated previously, the dialogue examples have been filtered to retain exactly one user-assistant exchange per sample, enforcing a single-turn instruction-response structure for all pairs.

\subsubsection{Roles and Their Purpose}
\begin{itemize}
\item \textbf{system}: An optional message that provides general context or high-level instruction, such as persona or tone guidelines.
\item \textbf{user}: Represents the prompt or question, serving as the input for which the model must generate a response.
\item \textbf{assistant}: Contains the model's response to the user's prompt; this serves as the target output during both training and evaluation.
\end{itemize}

\subsubsection{Dataset Schema}
Each data point is stored in JSON format as a dictionary with a single \texttt{messages} key, which holds a list of message objects. A typical single-turn user-assistant interaction is structured as follows:
\begin{lstlisting}[caption={User–assistant message pair}]
{"messages": [
    { "role": "user", "content": "<query>" },
    { "role": "assistant", "content": "<model output>" }
]}
\end{lstlisting}
This uniform structure facilitates both supervised fine-tuning and later evaluation using the same message format.

\subsection{Paraphrasing Samples}\label{sec:paraphrasing_samples}
As already mentioned in Section~\ref{sec:paraphrasing_a_dataset}, the covered approach operates with two different settings, namely \emph{paraphrased} and \emph{model-generated} paraphrasing levels. For the \emph{paraphrased} case, both the query (user message) and the generation (assistant message) are altered using \emph{OpenAI GPT-4o-mini} with a seed of $42$ and a temperature of $1$. The rest of the parameters are set to default values. The corresponding system prompt is illustrated below:
\begin{verbatim}
    You are a paraphrasing expert who is specialized in rewriting text 
    (questions, statements, etc.) without altering the content. 
    Keep in mind, that the meaning must not change after the paraphrasing. 
    Just output the paraphrased text without any additional information.
\end{verbatim}
The \emph{OpenAI} API also follows the message format shown in Subsection~\ref{subsec:message_format}. Consequently, when paraphrasing the example from Subsection~\ref{subsec:lima_dataset}, the \emph{OpenAI} API receives a system-user message pair object containing the previously illustrated system prompt and the text to be rephrased. Hence, all the samples and therefore, each user and assistant message from the filtered \emph{LIMA} dataset are individually sent to \emph{OpenAI} to be paraphrased. For example, the paraphrase of the the user message prompt shown in Subsection~\ref{subsec:lima_dataset} looks like:
\begin{verbatim}
    Are brain cells capable of moving? Specifically, I’m referring to
    long-distance migration, ideally occurring within the brain.
\end{verbatim}
Section~\ref{subsec:appendix_paraphrased_sample} additionally illustrates the paraphrases for the messages shown in Subsection~\ref{subsec:lima_dataset}. This procedure is then repeated for all samples $\si$ in $D$ to create the dataset $D_p$ first mentioned in Section~\ref{sec:paraphrasing_a_dataset}. To create the dataset $D_m$ for the \emph{model-generated} case, the paraphrased user messages from $D_p$ are input to the model under evaluation. For each paraphrased user message $s_{p_{i_P}}$, the model generates a new assistant message $s_{m_{i_G}}$.

\subsection{Prepare Model-generated Data Point}\label{subsec:prepare_model_input}
Before the user message can be picked up by the model, some modifications are made to the sample. Usually, the user message is embedded inside a form of tag that tells the model where the user message starts and ends. How these tags are formatted and applied differs from model to model in the corresponding training pipeline. In the case of \texttt{AMD-OLMo-1B-SFT}, it looks like:
\begin{verbatim}
    <|user|>\n
    Are brain cells capable of moving? 
    Specifically, I'm referring to long-distance migration, 
    ideally occurring within the brain.\n
    <|assistant|>\n
\end{verbatim}
The \texttt{<|user|>} tag tells the model where the user message starts. The \texttt{<|assistant|>} tag tells the model where the user message ends and that the model should start writing the response. In the space of HuggingFace, this kind of format is usually called \emph{chat template}. After applying this template, the text is tokenized and converted to a numerical format by a so-called \emph{tokenizer} to make it readable for the model. For \texttt{AMD-OLMo-1B-SFT}, the tokenized and converted sequence looks like the following:
\begin{verbatim}
     29, 93, 4537, 49651, 187, 6723, 3998, 1341, 7032, 273,
     4886, 32, 13658, 13, 309, 1353, 14339, 281, 1048, 14,
     19893, 10346, 13, 34243, 12952, 1561, 253, 3998, 15, 187,
     29, 93, 515, 5567, 49651, 187
\end{verbatim}
Each number represents one token; for example, the value $187$ encodes the term \texttt{\textbackslash n}. This sequence of tokens is used to generate a response, which also represents a sequence of tokens. This sequence is then decoded using the same \emph{tokenizer} as before. Section~\ref{subsec:appendix_model_generated_sample} illustrates the decoded answer for the example shown above.
\\\\
This process is then repeated for each original sample to generate the \emph{paraphrased} dataset $D_p$ and the \emph{model-generated} dataset $D_m$. These \emph{paraphrased} and \emph{model-generated} examples are stored on disk and picked up when needed to guarantee a level of determinism when operating on this newly created dataset. 
\fxnote{can I mention my own github repo? Maybe I can publish the dataset on HuggingFace and mention it here}

\subsection{Supervised Fine-Tuning Setup}
Although no additional \acrfull{sft} is performed in this work, all data samples are prepared according to established \acrshort{sft} conventions. This ensures full compatibility with existing model evaluation pipelines and facilitates the subsequent analysis of model behavior from an explainability perspective. Specifically, each sample is formatted as a user–assistant message pair, following the schema described in previous sections.
\\\\
\acrfull{sft} is a process in which a pretrained language model is further trained on a collection of task-specific examples, each consisting of an input (such as a user prompt) and an expected output (such as an assistant response). The goal is to optimize the model’s parameters so that its outputs closely match the ground truth responses, typically minimizing loss of cross-entropy. In the context of conversational models, \acrshort{sft} datasets are structured as lists of message objects, each message specifying a \texttt{role} (e.g., user or assistant) and a \texttt{content} field, as described in Section~\ref{subsec:message_format}. Formatting the data in this standardized way enables the fine-tuning process to teach the model not only to generate accurate responses, but also to adhere to the required conversational structure. As already mentioned, no further \acrshort{sft} is performed in this work, but all samples are formatted in this way to ensure full compatibility with existing pipelines and to support consistent evaluation.

\subsubsection{Open-Instruct}
In the context of this work, \emph{Open-Instruct} is mainly utilized to preprocess and structure the data according to established \acrshort{sft} conventions. Specifically, the framework offers utilities for converting raw user–assistant-message pairs such as those from the \emph{LIMA} dataset into the standardized chat template format (e.g., \texttt{<|user|>} and \texttt{<|assistant|>} tags) used by models like \texttt{AMD-OLMo-1B-SFT}. This ensures consistency between the data preparation procedure and the input expectations of the underlying model architecture.
\\\\
To preprocess the data for \acrfull{sft}, \emph{SFTDatasetProcessor} from \emph{Open-Instruct} is used. It allows a whole dataset to be tokenized and prepared by forming the corresponding input and output sequences and naturally integrating into the \emph{HuggingFace} ecosystem for \acrshort{sft}.

\section{Instance- and Gradient-based Explanation}
\fxnote{mention batch size of 1}

\subsection{High Dimensionality}\label{subsec:high_dimensionality}
\fxnote{mention gradient caching on disk and why it's not suitable for our case, also calculate potential disk usage}

\subsection{BM25-selected Samples}\label{subsec:setup_bm25}
\fxnote{explain that by comparing the first 20 gradients NxN, the most similar examples where in the main diagonal}
\fxnote{reduce execution time by selection top-n BM25 selected samples}

\subsection{Intermediate Results}\label{subsec:intermediate_results}

\fxnote{dot products (per layer) as intermediate results to compute gradient cosine similarity later on because full gradients have to be stored for each sample and each gradient is as big as the model size}

\fxnote{add execution times, server resources, etc.}