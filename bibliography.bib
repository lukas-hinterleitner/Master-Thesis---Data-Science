@book{lamport94,
    author = {Leslie Lamport},
    edition = {Second},
    note = {User's Guide and Reference Manual},
    publisher = {Addison-Wesley},
    title = {\LaTeX\ -- A Document Preparation System},
    year = {1994}
}

@misc{wang2023far,
    title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}, 
    author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
    year={2023},
    eprint={2306.04751},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{ivison2023camels,
    title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, 
    author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
    year={2023},
    eprint={2311.10702},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{OLMo,
    title={OLMo: Accelerating the Science of Language Models},
    author={DirkGroeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and A. Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hanna Hajishirzi},
    year={2024},
    url={https://api.semanticscholar.org/CorpusID:267365485},
    journal={arXiv preprint},
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Hammoudeh_2024,
   title={Training data influence analysis and estimation: a survey},
   volume={113},
   ISSN={1573-0565},
   url={http://dx.doi.org/10.1007/s10994-023-06495-7},
   DOI={10.1007/s10994-023-06495-7},
   number={5},
   journal={Machine Learning},
   publisher={Springer Science and Business Media LLC},
   author={Hammoudeh, Zayd and Lowd, Daniel},
   year={2024},
   month=mar, pages={2351–2403}
}


@misc{sutskever2014sequencesequencelearningneural,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.3215}, 
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{xiong2024explainableartificialintelligencexai,
      title={Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective}, 
      author={Haoyi Xiong and Xuhong Li and Xiaofei Zhang and Jiamin Chen and Xinhao Sun and Yuchen Li and Zeyi Sun and Mengnan Du},
      year={2024},
      eprint={2401.04374},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2401.04374}, 
}

@article{https://doi.org/10.1002/widm.1391,
    author = {Confalonieri, Roberto and Coba, Ludovik and Wagner, Benedikt and Besold, Tarek R.},
    title = {A historical perspective of explainable Artificial Intelligence},
    journal = {WIREs Data Mining and Knowledge Discovery},
    volume = {11},
    number = {1},
    pages = {e1391},
    keywords = {explainable AI, explainable recommender systems, interpretable machine learning, neural-symbolic reasoning},
    doi = {https://doi.org/10.1002/widm.1391},
    url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1391},
    eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1391},
    abstract = {Abstract Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision-making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge-based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural-symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human-understandable explainable systems. This article is categorized under: Fundamental Concepts of Data and Knowledge > Explainable AI Technologies > Artificial Intelligence},
    year = {2021}
}

@misc{european_commission_regulation_2016,
  abstract = {EU Datenschutzgrundverordnung (kurz: DS-GVO / auch: DSGVO) im Orginal mit allen Sprachversionen},
  added-at = {2020-08-20T11:33:21.000+0200},
  author = {{European Commission}},
  biburl = {https://www.bibsonomy.org/bibtex/243a2175512dc8b9d8855fa7a763cdc3e/zotero},
  interhash = {c7b667cac6031282160a9e94d5a118f8},
  intrahash = {43a2175512dc8b9d8855fa7a763cdc3e},
  keywords = {ddm en m1.3 open_access},
  publisher = {European Commission},
  timestamp = {2020-08-20T11:33:21.000+0200},
  title = {Regulation ({EU}) 2016/679 of the {European} {Parliament} and of the {Council} of 27 {April} 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing {Directive} 95/46/{EC} ({General} {Data} {Protection} {Regulation}) ({Text} with {EEA} relevance)},
  url = {https://eur-lex.europa.eu/eli/reg/2016/679/oj},
  year = 2016
}

@article{ALI2023101805,
    title = {Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
    journal = {Information Fusion},
    volume = {99},
    pages = {101805},
    year = {2023},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2023.101805},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
    author = {Sajid Ali and Tamer Abuhmed and Shaker El-Sappagh and Khan Muhammad and Jose M. Alonso-Moral and Roberto Confalonieri and Riccardo Guidotti and Javier {Del Ser} and Natalia Díaz-Rodríguez and Francisco Herrera},
    keywords = {Explainable Artificial Intelligence, Interpretable machine learning, Trustworthy AI, AI principles, Post-hoc explainability, XAI assessment, Data Fusion, Deep Learning},
    abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.}
}

@misc{zhao2023explainabilitylargelanguagemodels,
      title={Explainability for Large Language Models: A Survey}, 
      author={Haiyan Zhao and Hanjie Chen and Fan Yang and Ninghao Liu and Huiqi Deng and Hengyi Cai and Shuaiqiang Wang and Dawei Yin and Mengnan Du},
      year={2023},
      eprint={2309.01029},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.01029}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{openai_2023, 
    url={https://openai.com/index/language-models-can-explain-neurons-in-language-models}, 
    title={Language models can explain neurons in language models}, 
    author={OpenAI}, 
    year={2023}, 
    month={May},
    note = {Accessed: 09-01-2025},
    howpublished={\url{https://openai.com/index/language-models-can-explain-neurons-in-language-models}}
} 

@misc{openai_2024_gpt4o_mini, 
    url={https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}, 
    title={GPT-4o mini: advancing cost-efficient intelligence}, 
    author={OpenAI}, 
    year={2024}, 
    month={July},
    note = {Accessed: 09-01-2025},
    howpublished={\url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}
} 

@misc{meta_ai_llama_3_1, 
    url={https://ai.meta.com/blog/meta-llama-3-1/}, 
    title={Introducing Llama 3.1: Our most capable models to date}, 
    author={Meta AI}, 
    year={2024}, 
    month={July},
    note = {Accessed: 10-01-2025},
    howpublished={\url{https://ai.meta.com/blog/meta-llama-3-1/}}
}

@misc{openai_2018_generative_pre_training,
    url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
    title={Improving Language Understanding by Generative Pre-Training},
    author={Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
    year={2018},
    note={Accessed: 10-01-2025},
    howpublished={\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}}
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@ARTICLE{Rosenblatt1958-mn,
  title     = "The perceptron: a probabilistic model for information storage
               and organization in the brain",
  author    = "Rosenblatt, F",
  journal   = "Psychol. Rev.",
  publisher = "American Psychological Association (APA)",
  volume    =  65,
  number    =  6,
  pages     = "386--408",
  month     =  nov,
  year      =  1958,
  keywords  = "PERCEPTION",
  language  = "en"
}

@misc{wang2019gluemultitaskbenchmarkanalysis,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.07461}, 
}

@misc{koh2020understandingblackboxpredictionsinfluence,
      title={Understanding Black-box Predictions via Influence Functions}, 
      author={Pang Wei Koh and Percy Liang},
      year={2020},
      eprint={1703.04730},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1703.04730}, 
}

@misc{jin2020bertreallyrobuststrong,
      title={Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}, 
      author={Di Jin and Zhijing Jin and Joey Tianyi Zhou and Peter Szolovits},
      year={2020},
      eprint={1907.11932},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11932}, 
}

@misc{wang2022semattacknaturaltextualattacks,
      title={SemAttack: Natural Textual Attacks via Different Semantic Spaces}, 
      author={Boxin Wang and Chejian Xu and Xiangyu Liu and Yu Cheng and Bo Li},
      year={2022},
      eprint={2205.01287},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01287}, 
}

@misc{guo2021fastifscalableinfluencefunctions,
      title={FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging}, 
      author={Han Guo and Nazneen Fatema Rajani and Peter Hase and Mohit Bansal and Caiming Xiong},
      year={2021},
      eprint={2012.15781},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.15781}, 
}

@misc{grosse2023studyinglargelanguagemodel,
      title={Studying Large Language Model Generalization with Influence Functions}, 
      author={Roger Grosse and Juhan Bae and Cem Anil and Nelson Elhage and Alex Tamkin and Amirhossein Tajdini and Benoit Steiner and Dustin Li and Esin Durmus and Ethan Perez and Evan Hubinger and Kamilė Lukošiūtė and Karina Nguyen and Nicholas Joseph and Sam McCandlish and Jared Kaplan and Samuel R. Bowman},
      year={2023},
      eprint={2308.03296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.03296}, 
}

@misc{wang2024datashapleytrainingrun,
      title={Data Shapley in One Training Run}, 
      author={Jiachen T. Wang and Prateek Mittal and Dawn Song and Ruoxi Jia},
      year={2024},
      eprint={2406.11011},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11011}, 
}

@article{bm25,
    author = {Robertson, Stephen and Zaragoza, Hugo},
    title = {The Probabilistic Relevance Framework: BM25 and Beyond},
    year = {2009},
    issue_date = {April 2009},
    publisher = {Now Publishers Inc.},
    address = {Hanover, MA, USA},
    volume = {3},
    number = {4},
    issn = {1554-0669},
    url = {https://doi.org/10.1561/1500000019},
    doi = {10.1561/1500000019},
    abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
    journal = {Found. Trends Inf. Retr.},
    month = apr,
    pages = {333–389},
    numpages = {57}
}

@misc{lin2024tokenwiseinfluentialtrainingdata,
      title={Token-wise Influential Training Data Retrieval for Large Language Models}, 
      author={Huawei Lin and Jikai Long and Zhaozhuo Xu and Weijie Zhao},
      year={2024},
      eprint={2405.11724},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.11724}, 
}

@misc{yeh2022betterlanguagedatainfluence,
      title={First is Better Than Last for Language Data Influence}, 
      author={Chih-Kuan Yeh and Ankur Taly and Mukund Sundararajan and Frederick Liu and Pradeep Ravikumar},
      year={2022},
      eprint={2202.11844},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.11844}, 
}

@misc{you2020largebatchoptimizationdeep,
      title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes}, 
      author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
      year={2020},
      eprint={1904.00962},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.00962}, 
}

@misc{dl4nlp,
  author        = {Erion \c{C}ano and Benjamin Roth and Andreas Stephan},
  title         = {Lecture notes in Deep Learning for Natural Language Processing},
  month         = {February},
  year          = {2023},
  publisher     = {Faculty of Computer Science, University of Vienna}
}

@misc{press2017usingoutputembeddingimprove,
      title={Using the Output Embedding to Improve Language Models}, 
      author={Ofir Press and Lior Wolf},
      year={2017},
      eprint={1608.05859},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1608.05859}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{curtarelli2023generalizedvectorizationinverse,
      title={On the generalized vectorization and its inverse}, 
      author={Vitor Curtarelli},
      year={2023},
      eprint={2308.07928},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2308.07928}, 
}

@misc{barshan2020relatifidentifyingexplanatorytraining,
      title={RelatIF: Identifying Explanatory Training Examples via Relative Influence}, 
      author={Elnaz Barshan and Marc-Etienne Brunet and Gintare Karolina Dziugaite},
      year={2020},
      eprint={2003.11630},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2003.11630}, 
}

@inproceedings{
    sui2021representer,
    title={Representer Point Selection via Local Jacobian Expansion for Post-hoc Classifier Explanation of Deep Neural Networks and Ensemble Models},
    author={Yi Sui and Ga Wu and Scott Sanner},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=Wl32WBZnSP4}
}